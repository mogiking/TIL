\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Lecture note}
\author{송진호 }
\date{March 2021}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\begin{document}
\begin{flushright}
\textbf{2021511019 Jinho Song }
\end{flushright}
\section{Introduction to system of Linear Algebra}

\subsection{Linear Algebra}
{\
\textbf{Definition} : A Linear equation in the variables X$_1$,$\cdots$,X$_n$ is an equation that can be written as
}
\[
    a_1X_1 + \cdots + a_nX_n = b
\]

{\
\\
\textbf{Examples} : which of the following equation are Linear

\begin{enumerate}
\item {$4X_1 - 5X_2 + 2 = X_1$}
\item {$X_2 = 2(\sqrt{6} - X_1 ) + X_3 $}
\item {$4X_1 - 6X_2 = X_1X_2$}
\item {$X_2 = 2\sqrt{X_1}-7$}
\end{enumerate}
}

\subsection*{Linear System}
{\
\textbf{Definition}: A linear system is a collection of one or more linear equation involving the same set of variables $X_1, \cdots , X_n$. A solution of a linear system is a list $(s_1, \cdots , s_n)$ are substituted for $X_1 , \cdots , X_n$ respectively. 
}

{\
\\
\textbf{Examples}:


\begin{align*} 
(1)  &              & (2)        &      & (3)  &\\
X_1 + X_2 &= 1      & X_1 - 2X_2 &= -3  &   2X_1 + X_2 &= 1  \\
-X_1 + X_2 &= 0     & 2X_1 - 4X_2 &= 8  &   -4X_1 - 2X_2 &= -2
\end{align*}
}
(1) Only one solution
(2) No solution
(3) Infinitely many solution


{\
\\
\textbf{Theorem} : A Linear system has either
\begin{itemize}
\item{no solution}
\item{one unique solution}
\item{infinitely many solution }
\end{itemize}

\noindent \textbf{Definition} : A system is consistent if a solution exists
}

\subsection{How to solve a linear system}
{\
\noindent \textbf{Strategy} : Replace a system with an equivalent system which is easier to solve. 
\\
\noindent \textbf{Definition} : Linear systems are equivalent if they have the same set of solutions.
\\
\noindent \textbf{Example} :
\begin{align*} 
    X_1 + X_2 &= 1   &                                                                      & & X_1 + X_2 &= 1  \\
    -X_1 + X_2 &= 0  &\raisebox{3ex}[0pt][0pt]{ $\underrightarrow{R_2 \leftarrow R_2 + R_1}$ }& &      2X_2 &= 1
\end{align*}


\begin{align*}
    &\text{Augmented matrix:}  & X_1 - 2X_2 &= -1 \\
    &                           & -X_1 + 3X_2 &= 3
\end{align*}

\section{Solution of linear systems via row reduction}
{
\begin{itemize}
    \item Gaussian elimination : Row reduce to echelon form (\textbf{REF})
    \item Gauss-Jordan elimination : Row reduce to reduced echelon form (\textbf{RREF})
\end{itemize}
}

\subsection{General solution in parametric form}
{
\[
\begin{bmatrix}[ccccc|c]
    1 & 6 & 0 & 3 & 0 & 0 \\
    0 & 0 & 1 & -8& 0 & 5 \\
    0 & 0 & 0 & 0 & 1 & 7
\end{bmatrix}
\]
\begin{align*}
     X_1 - 6X_2 + 3X_4 &= 0 \\
     X_3 + -8X_4 &= 5 \\
     X_5 &=7
\end{align*}
\textbf{General Solution in parametric form:}
\begin{align*}
     X_1 &= -6X_2 -3X_4 \\
     X_2 &= X_2 \quad \textit{(free variable)}\\
     X_3 &= 8X_4 + 5 \\
     X_4 &= X_4 \quad \textit{(free variable)}\\
     X_5 &= 7
\end{align*}
}
\subsection{Questions of existence and awareness}

{
Linear system is \textbf{consistent} if and only if an REF of the augmented matrix has no row of form
\[
\begin{bmatrix}[ccccc|c]
    0 & 0 & 0 & \cdots & 0 & b 
\end{bmatrix}
 \quad \textit{($b \neq 0$)}
\]
If consistent, \\ One unique solution \textit{(no free variable)} \\  Infinitely many solutions \textit{(at least one free variables)}

\text{Summary:}
\begin{itemize}
    \item{Each linear systems corresponds to an augmented matrix}
    \item{From the Gaussian elimination , we can read off, whether the system has no, one, or infinitely many solutions}
    \item{We can further row reduce to reduced echelon form}
    \begin{itemize}
        \item{General solution in parametric form}
        \item{This form is unique}
    \end{itemize}
    \item{The recipe to solve linear systems}
    \begin{enumerate}
        \item {Augmented matrix}
        \item {Gauss-Jordan elimination}
        \item {Declare pivot \& free variables, state the solutions in terms of free variables}
    \end{enumerate}x`
\end{itemize}
}

\section{Linear system - Column picture }

A vector equation
\[
x_1\vec{a_1} + \cdots + x_m\vec{a_m} = \vec{b}
\]
has the same solution set as the linear system with augmented matrix
\[
\begin{bmatrix}[ccc|c]
    \vec{a_1} & \cdots & \vec{a_m} & \vec{b}
\end{bmatrix}
\]
In particular, $\vec{b}$ can be generated by a linear combination of $\vec{a_1}, \cdots , \vec{a_m}$ if and if only, linear system is consistent
}

\subsection{The span of a set vectors}
\textbf{Definition : }
The span of vectors $\vec{v_1}, \cdots , \vec{v_m}$ is the set of all their linear combinations. We denote it by $span \{\vec{v_1}, \cdots , \vec{v_m}\}$

\textbf{Example :}
\[
span
\begin{bmatrix}
    2\\
    -1 \\
    1
\end{bmatrix}
\begin{bmatrix}
    4\\
    -2\\
    1
\end{bmatrix}
\]\[
\begin{bmatrix}
    2\\
    -1\\
    1
\end{bmatrix}
=
a
\begin{bmatrix}
    4\\
    -2\\
    1
\end{bmatrix}
\]
if $a$ exists, span is line. However, $a$ does not exists, so span is plane.

\subsection{Matrix operations}
\subsubsection{Matrix times matrix}
\[
AB = 
\begin{bmatrix}
    A
\end{bmatrix}
\begin{bmatrix}
    \vec{b_1} & \cdots & \vec{b_m}
\end{bmatrix}
=
\begin{bmatrix}
    A\vec{b_1} & \cdots & A\vec{b_m}
\end{bmatrix}
\]
Each column of AB is linear combination of the columns of A with weight given by the corresponding column of B
\[
(AB)\vec{x} = A(B\vec{x})
\]
\begin{itemize}
    \item $A(BC) = (AB)C$
    \item $A(B+C) = AB+AC$
    \item $(A+B)C = AC + BC$
\end{itemize}


\section{ Transpose of a matrix }
{\

\textbf{Definition} : The transpose $A^T$ of a matrix $A$ is the matrix whose columns are formed from the corresponding row of $A$.

\textbf{Example} : 
}
\[
\begin{bmatrix}
    a & b  \\
    c & d  \\
    e & f 
\end{bmatrix}^T
= 
\begin{bmatrix}
    a & c & e \\
    b & d & f 
\end{bmatrix}
\]

{\

\textbf{Theorem} : 
\begin{itemize}
    \item $(A^T)^T = A$
    \item $(A+B)^T = A^T+B^T$
    \item $(AB)^T = B^TA^T$
    \item $(ABC)^T = C^TB^TA^T$
\end{itemize}

}

\section{ LU Decomposition }
{\

\textbf{Definition} : An elementary matrix is one that is obtained by performing a single elementary row operation on an identity matrix.


}

\[
A =
\begin{bmatrix}
    2 & 1  \\
    4 & -6  
\end{bmatrix}
\rightarrow
\begin{bmatrix}
    1 & 0 \\
    -2 & 1
\end{bmatrix}
\begin{bmatrix}
    2 & 1 \\
    4 & -6
\end{bmatrix}
=
\begin{bmatrix}
    2 & 1 \\
    0 & -8
\end{bmatrix}
\]

\[
A=
\begin{bmatrix}
    1 & 0 \\
    2 & 1
\end{bmatrix}^{[L]}
\begin{bmatrix}
    2 & 1 \\
    0 & -8
\end{bmatrix}^{[U]}
\]
{\

A = LU is known as the\textbf{ LU }decomposition of A. 

(\textbf{L}: lower triangular, \textbf{U}: upper triangular)

}

{\

\textbf{Remark}

\begin{itemize}
    \item Once we have $A = LU$, it is simple to solve $A\vec{x}=\vec{b}$
    \item Not always factor every matrix $A$ as $A=LU$
\end{itemize}

\textbf{Definition}: A permutation matrix is one that is obtained by performing row exchanges on an identity matrix.

\textbf{Theorem}: For any matrix A, there is a permutation matrix $P$ such that $PA=LU$

}

\section{The inverse of matrix}

{


\textbf{Definition}: 
An $n \times n$ matrix $A$ is invertible if there is a matrix $B$ such that $AB=BA=I$. 
In that case, B is inverse of A, and we write $A^{-1}=B$

\textbf{Theorem}: Let $A$ be invertible, Then the system  $A\vec{x}=\vec{b}$ has the unique solution 
 $\vec{x}=A^{-1}\vec{b}$
 
 
\textbf{Remark}

\begin{itemize}
    \item If $A\vec{x}=\vec{b}$ does not have the unique solution, then A is not invertible.
\end{itemize}

}
\subsection{Recipe for computing the inverse}

{

\begin{enumerate}
\item Form the augmented matrix $[A|I]$
\item Compute the reduced echelon form(Gauss-Jordan elimination)
\item If $A$ is invertible, the result is of form $[I|A^-1]$
\end{enumerate}
}
{


\textbf{Example}: 

Find the inverse of $A = \begin{bmatrix}
    2 & 0 & 0  \\
    -3 & 0 & 1  \\
    0 & 1 & 0  \\
\end{bmatrix}$
\[
\begin{bmatrix}[ccc|ccc]
    2 & 0 & 0 & 1 & 0 & 0  \\
    -3 & 0 & 1 & 0 & 1 & 0  \\
    0 & 1 & 0 & 0 & 0 & 1  \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}[ccc|ccc]
    1 & 0 & 0 & 1/2 & 0 & 0  \\
    0 & 1 & 0 & 0 & 0 & 1  \\
    0 & 0 & 1 & 3/2 & 1 & 0  \\
\end{bmatrix}
\]

\[
A^{-1} = \begin{bmatrix}
    1/2 & 0 & 0  \\
    0 & 0 & 1  \\
    3/2 & 1 & 0  \\
\end{bmatrix}
\]

\textbf{Theorem} : Suppose $A$ and $B$ are invertible,
\begin{itemize}
    \item $A^{-1}$ is invertible, $(A^{-1})^{-1} = A$
    \item $A^T$ is invertible, and $(A^T)^{-1} = (A^{-1})^{T}$ 
    \item $(A^{-1})^{T}A^T = (AA^{-1})^T = I^T = I$
    \item $(AB)^{-1} = B^{-1}A^{-1}$
\end{itemize}


\textbf{Theorem} : Let $A$ be an $n \times n$ matrix. Then the following statements are equivalent.
\begin{itemize}
    \item $A$ is invertible.
    \item $A$ is row equivalent to $I$
    \item $A$ has $n$ pivots
    \item $A\vec{x}=\vec{b}$ has a unique solution
\end{itemize}

* Matrice that are not invertible are often called singular.
}

\subsection{LU decomposition vs. matrix inverse}
{
\begin{itemize}
    \item In many applications, we don't just solve $A\vec{x}=\vec{b}$ for a single $\vec{b}$, but for many different $\vec{b}$s.
    \item That is why LU decomposition saves us from repeating lots of computation in comparison with Gaussian elimination on $[ A | \vec{b} ]$
    \item Complexity of LU decomposition is $O(n)$. Complexity of inverse is $O(n^2)$.
\end{itemize}
}

\section{Vector spaces and subspaces}

\subsection{Vector spaces}
{

\textbf{Definition}: 
A vector space is a non-empty set $V$ of vectors, which may be \textbf{added} and \textbf{scaled}.

\begin{itemize}
    \item Axioms ($u,v,w \in V$,  $c,d \in \mathbb{R}$).
    \begin{enumerate}
        \item $u+v \in V$
        \item $u+v = v + u$
        \item $(u+v)+w = u+(v+w)$
        \item $\exists\mathbf{0} \in V $, s.t, $u+\mathbf{0}=u$ $\forall u \in V$
        \item $\exists u \in V$, s.t, $u+(-u)=\mathbf{0}$
        \item $cu \in V$
        \item $c(u+v) = cu + cv$
        \item $(c+d)u = cu + du$
        \item $(cd)u = c(du)$
        \item $1\cdot u = u$
    \end{enumerate}
\end{itemize}

}

\subsection{Subspaces}
{

\textbf{Definition}: 
A subset W of a vector space V is a subspace, if W is itself a vector space. Since the rules like associativity, commutativity, and distributivity stll hold, we only need to check the followings.
\[
W \subseteq V \text{  is a subspace of } V \text{ if}
\]

\begin{enumerate}
    \item $W$ contains the zero vector
    \item $W$ is closed under addition
    \item $W$ is closed under scaling
\end{enumerate}


\textbf{Theorem} : If $A \in \mathbb{R}^{m \times n}$, then $Nul(A)$ is a subspace of $\mathbb{R}^{n}$
\begin{itemize}
    \item $\vec{0} \in Nul(A)$ since, $A\vec{0} = \vec{0}$
    \item $\vec{x},\vec{y} \in Nul(A) \rightarrow  A \vec{x} = A \vec{y} = \vec{0} $ \\ Then $\vec{x}+\vec{y} \in Nul(A)$ since $A(\vec{x}+\vec{y}) = A\vec{x} = A\vec{y} = \vec{0}$
    \item $\vec{x} \in Nul(A) \rightarrow  \gamma\vec{x} \in Nul(A) $ \\ since $ \gamma A \vec{x} = \gamma \vec{0} = \vec{0}$
\end{itemize}

}

\subsection{Linear independence of matrix columns}
{
Each linear dependence relation of columns of A corresponds to a nontrival solution to Ax=0


\textbf{Theorem} : Let $A$ be an $m \times n$ matrix. The columns of $A$ are linearly independent
\begin{itemize}
    \item $A \vec{x} = \vec{0}$ has only the solution $\vec{x}=\vec{0}$.
    \item $Nul(A) = \{\vec{0}\}$
    \item $A$ has $n$ pivots.
\end{itemize}
}

\subsection{A basis of a vector space}
{
\textbf{Definition} : A set of vectors $\{\vec{v_1},\cdots,\vec{v_p}\}$ in $V$ is a basis of $V$ if
\begin{enumerate}
    \item $V = span\{\vec{v_1},\cdots,\vec{v_p}\}$ 
    \item $\{\vec{v_1},\cdots,\vec{v_p}\}$ are linearly independent
\end{enumerate}

\textbf{Theorem} : If $S$ is a basis of a vector space V, then every vector in V has exactly one representation as a linear combinations of elements of S.

\textbf{Theorem} : If $V$ has a basis with $n$ elements, then every set of $m$ vectors which has more then $n$ elements is linearly dependent. ($m>n$)

\textbf{Theorem} :  If $V$ has a basis with $n$ elements, then every set of vectors with fewer than $n$ elements does not span $V$

\textbf{Theorem} : If $V$ has a basis with $n$ elements, then all bases of $V$ have the same number of elements.

\textbf{Definition} : The number of elements in a basis of a vectorspace is called dimension.

\textbf{Theorem} : Suppose that $V$ has dimension $d$,
\begin{enumerate}
    \item A set of $d$ vectors in $V$ are a basis if they span $V$.
    \item A set of $d$ vectors in $V$ are a basis if they are linearly independent.
\end{enumerate}

}
\subsection{The Four fundamental subspaces}
{
\textbf{Definition} : \\The row space of $A$ is $Col(A^T)$.\\The left nullspace of $A$ is $Nul(A^T)$

\textbf{Theorem} : Fundamental theorem of linear algebra(FTLA)\\
Let $A \in \mathbb{R}^{m \times n}$ of rank $r$.
\begin{itemize}
    \item $dim Col(A) = r$
    \item $dim Col(A^T) = r$
    \item $dim Nul(A) = n-r$
    \item $dim Nul(A^T) = m-r$
\end{itemize}

}

\section{Linear transforms}
{
\textbf{Definition} : A map $T: V \mapsto W$ is a linear transform if \\
$T(c\vec{x}+d\vec{y}) = cT(\vec{x})+dT(\vec{y})$, $\vec{x},\vec{y} \in V$ , $c,d \in \mathbb{R}$
}

\subsection{Representing linear maps by matrices}
{
\textbf{Definition} : Let $\vec{x_1}, \cdots , \vec{x_n}$ be a basis for $V$, and $\vec{y_1}, \cdots , \vec{y_m}$ be a basis for $W$.\\
The matrix representing T with respect to these bases,
\begin{itemize}
    \item has $n$ column (one for each $\vec{x_j}$)
    \item the j-th column has $m$ entries $a_{1j},a_{2j}, \cdots , a_{mj}$ determined by
    \[
    T(\vec{x_j}) = a_{1j}\vec{y_1} + a_{2j}\vec{y_2} + \cdots + a_{mj}\vec{y_m}
    \]
\end{itemize}

}

\section{Orthogonality}
{


\textbf{Definition} : 
The inner product of $\vec{v},\vec{w} \in \mathbb{R}^{n}$
\[
\vec{v} \cdot \vec{w} = \vec{v}^T\vec{w} = v_1w_1 + \cdots + v_nw_n
\]

\textbf{Definition} : 
The norm of a vector $\vec{v}\in \mathbb{R}^{n}$ is
\[
\norm{\vec{v}} \triangleq \sqrt{\vec{v} \cdot \vec{v}}
\]
The distance between $\vec{v},\vec{w}$ is
\[
dist(\vec{v},\vec{w}) \triangleq \norm{\vec{v}-\vec{w}}
\]

\textbf{Definition} : 
The $\vec{v},\vec{w} \in \mathbb{R}^{n}$ are orthogonal if
\[
\vec{v} \cdot \vec{w} = 0
\]

\textbf{Theorem}:
Suppose that $\vec{v_1}, \cdots, \vec{v_n}$ are non zero, and pairwise orthogonal. Then, they are independent.
\[\]

\textbf{Definition} : 
Let $W$ be a subspace of  $\mathbb{R}^{n}$ and $\vec{v} \in \mathbb{R}^{n}$
\begin{itemize}
    \item $\vec{v}$ is orthogonal to $W$, if $\vec{v} \cdot \vec{w} = 0$ for all $\vec{w} \in W$
    \item Another subspace $V$ is orthogonal to $W$, if every vector in $V$ is orthogonal to $W$.
    \item The orthogonal complement of $W^\perp$ of all vectors that are orthogonal to $W$
\end{itemize}


\textbf{Theorem}:\\
$Nul(A)$ is orthogonal to $Col(A^\perp)$\\
$Nul(A^\perp)$ is orthogonal to $Col(A)$

\[\]

}


\subsection{Orthogonal Bases}
{
\textbf{Definition} : 
A basis $\vec{v_1}, \cdots , \vec{v_n}$ of vector space $V$ is an orthgonal basis if the vectors are pairwise orthogonal.

}


\subsection{Orthogonal Projection}
{
\textbf{Definition} : 
The orthogonal projection of vector $\vec{x}$ onto vector $\vec{y}$ is
\[
\hat{x} \triangleq (\frac{\vec{x} \cdot \vec{y}}{\vec{y} \cdot \vec{y}})\vec{y}
\]

}


\subsection{Orthogonal Projection on subspaces}
{
\textbf{Theorem} : 
Let $W$ be a subspace of $\mathbb{R}^n$. Then, each $\vec{x} \in \mathbb{R}^n$ can be uniquely written as

\begin{align*}
                        \vec{x} &= \hat{x} + \hat{x}^\perp              \\
    \text{where } \hat{x} \in W &\text{and } \hat{x}^\perp \in W^\perp
\end{align*}

\textbf{Definition} : 
Let $\vec{v_1}, \cdots , \vec{v_n}$ be an orthogonal basis of $W \in \mathbb{R}^n$ \\
Then the projection map $\pi_w : \mathbb{R}^n \rightarrow \mathbb{R}^n$ is given by
\[
\vec{x} \mapsto \hat{x} = \sum_{i=1}^{n} (\frac{\vec{x} \cdot \vec{v_i}}{\vec{v_i} \cdot \vec{v_i}})\vec{v_i}
\]
and $\pi_w$ is linear.\\
The matrix $P$ representing $\pi_w \text{  }w,r,t$ the standard basis is the projection matrix.

}

\subsection{Normal equation}
{
\textbf{Theorem} : 
$\hat{x}$ is a least square solution of $A\vec{x}=\vec{b}$
\[
A^TA\hat{x} = A^T\vec{b}
\]
$\hat{x}$ is a LS solution of  $A\vec{x}=\vec{b}$
\begin{itemize}
    \item $(A\vec{x} - \vec{b)}$ is as small as possible
    \item $(A\vec{x} - \vec{b)} \perp Col(A)$ 
    \item $(A\vec{x} - \vec{b)} \in NUL(A^T)$
    \item $A^T(A\vec{x} - \vec{b)} = 0$ 
    \item $A^TA\hat{x} = A^T\vec{b}$ 
\end{itemize}
}

\section{QR decomposition}
{
Let $A$ be an $m \times n$ matrix of rank $n$. \\
Then, the QR decomposition $A=QR$\\
where $Q \in \mathbb{R}^{m \times n}$ with orthonormal columns,\\
$R$ is upper-triangular, $n \times n$, invertible.\\

$\hat{x}$ is a LS solution of  $A\vec{x}=\vec{b} \Longleftrightarrow R\hat{x}=Q^T\vec{b}$
}

\section{Determinant}

{
\textbf{Definition} : 
The determinant is characterized by
\begin{itemize}
    \item the normalization $det(I)=1$
    \item and how it is affected by elementary row operations
    \begin{itemize}
        \item (Replacement) Does not change the Det.
        \item (Interchange) Reverse the sign of the Det.
        \item (Scaling) Multiplies the Det with $s$.
    \end{itemize}
\end{itemize}

}
\section{Eigenvectors}

{
\textbf{Definition} : 
An eigenvector of $A$ is a nonzero $\vec{x}$ such that $A\vec{x}= \lambda\vec{x}$ for some scalar $\lambda$

\textbf{Theorem} : 
IF $\vec{x_1}, \cdots , \vec{x_n}$ are eigenvectors of $A$ corresponding to different eigenvalues, then they are independent.

}

\end{document}
